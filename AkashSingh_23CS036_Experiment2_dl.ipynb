{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkkiNikumbh/DL-EXPERIMENTS/blob/main/AkashSingh_23CS036_Experiment2_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "D6KPrLiO-xaw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. LOADING DATASET"
      ],
      "metadata": {
        "id": "swozNxcD-iIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "val_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset, batch_size=64, shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset=val_dataset, batch_size=64, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "y1zix_rR-27p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb722e4a-6627-445a-848a-c4aaf6a4b2da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 339kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.27MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.NUMPY PREPROCESSING UTILITIES"
      ],
      "metadata": {
        "id": "xqV-jK0DCMCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ONE HOT ENCODING\n",
        "def one_hot_encode(labels, num_classes=10):\n",
        "    one_hot = np.zeros((labels.shape[0], num_classes))\n",
        "    for i in range(labels.shape[0]):\n",
        "        one_hot[i, labels[i]] = 1\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "HOppzFM4CZwC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ACCURACY\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    correct = np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1)\n",
        "    return np.mean(correct)"
      ],
      "metadata": {
        "id": "9BregCuvCcBZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.ACTIVATION FUNCTIONS AND THEIR DERIVATIVES"
      ],
      "metadata": {
        "id": "GLG2PFetCqCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.RELU\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "#2.SIGMOID\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "#3.TANH\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "def tanh_derivative(z):\n",
        "    return 1 - np.tanh(z) ** 2\n",
        "\n",
        "#4.SOFTMAX\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "rIjq3R4tCv4R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. NEURAL NETWORK CLASS"
      ],
      "metadata": {
        "id": "1OaIopd7DEvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activations, learning_rate):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activations = activations\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.z_values = {}\n",
        "        self.a_values = {}\n",
        "        self.gradients = {}\n",
        "\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            self.weights[i] = np.random.randn(\n",
        "                layer_sizes[i], layer_sizes[i + 1]\n",
        "            ) * 0.01\n",
        "            self.biases[i] = np.zeros((1, layer_sizes[i + 1]))\n",
        "    def forward(self, X):\n",
        "      self.a_values[0] = X\n",
        "\n",
        "      for i in range(len(self.activations)):\n",
        "          Z = np.dot(self.a_values[i], self.weights[i]) + self.biases[i]\n",
        "          self.z_values[i + 1] = Z\n",
        "\n",
        "          if self.activations[i] == \"relu\":\n",
        "              A = relu(Z)\n",
        "          elif self.activations[i] == \"sigmoid\":\n",
        "              A = sigmoid(Z)\n",
        "          elif self.activations[i] == \"tanh\":\n",
        "              A = tanh(Z)\n",
        "          elif self.activations[i] == \"softmax\":\n",
        "              A = softmax(Z)\n",
        "\n",
        "          self.a_values[i + 1] = A\n",
        "\n",
        "      return A\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "      epsilon = 1e-8\n",
        "      loss = -np.sum(y_true * np.log(y_pred + epsilon)) / y_true.shape[0]\n",
        "      return loss\n",
        "    def backward(self, y_true):\n",
        "      m = y_true.shape[0]\n",
        "      L = len(self.activations)\n",
        "\n",
        "      dZ = self.a_values[L] - y_true\n",
        "\n",
        "      for i in reversed(range(L)):\n",
        "          dW = np.dot(self.a_values[i].T, dZ) / m\n",
        "          dB = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "          self.gradients[\"W\" + str(i)] = dW\n",
        "          self.gradients[\"B\" + str(i)] = dB\n",
        "\n",
        "          if i > 0:\n",
        "              if self.activations[i - 1] == \"relu\":\n",
        "                  dZ = np.dot(dZ, self.weights[i].T) * relu_derivative(self.z_values[i])\n",
        "              elif self.activations[i - 1] == \"sigmoid\":\n",
        "                  dZ = np.dot(dZ, self.weights[i].T) * sigmoid_derivative(self.z_values[i])\n",
        "              elif self.activations[i - 1] == \"tanh\":\n",
        "                  dZ = np.dot(dZ, self.weights[i].T) * tanh_derivative(self.z_values[i])\n",
        "    def update_parameters(self):\n",
        "      for i in range(len(self.weights)):\n",
        "          self.weights[i] -= self.learning_rate * self.gradients[\"W\" + str(i)]\n",
        "          self.biases[i] -= self.learning_rate * self.gradients[\"B\" + str(i)]\n",
        "\n",
        "    def predict(self, X):\n",
        "      return self.forward(X)\n",
        "    def evaluate(self, X, y_true):\n",
        "      y_pred = self.forward(X)\n",
        "      return compute_accuracy(y_true, y_pred)"
      ],
      "metadata": {
        "id": "rqiQSK0qDL07"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.TRAINING LOOP"
      ],
      "metadata": {
        "id": "tqx5ScYDDohQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs):\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_losses = []\n",
        "        train_accuracies = []\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.cpu()\n",
        "            labels = labels.cpu()\n",
        "\n",
        "            images_np = images.numpy()\n",
        "            labels_np = labels.numpy()\n",
        "\n",
        "            images_np = images_np.reshape(images_np.shape[0], -1)\n",
        "            images_np = images_np / 1.0  # already normalized by ToTensor()\n",
        "\n",
        "            labels_oh = one_hot_encode(labels_np)\n",
        "\n",
        "            outputs = model.forward(images_np)\n",
        "            loss = model.compute_loss(labels_oh, outputs)\n",
        "\n",
        "            model.backward(labels_oh)\n",
        "            model.update_parameters()\n",
        "\n",
        "            train_losses.append(loss)\n",
        "            train_accuracies.append(compute_accuracy(labels_oh, outputs))\n",
        "\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for images, labels in val_loader:\n",
        "            images = images.cpu()\n",
        "            labels = labels.cpu()\n",
        "\n",
        "            images_np = images.numpy()\n",
        "            labels_np = labels.numpy()\n",
        "\n",
        "            images_np = images_np.reshape(images_np.shape[0], -1)\n",
        "            images_np = images_np / 1.0\n",
        "\n",
        "            labels_oh = one_hot_encode(labels_np)\n",
        "\n",
        "            outputs = model.forward(images_np)\n",
        "            val_losses.append(model.compute_loss(labels_oh, outputs))\n",
        "            val_accuracies.append(compute_accuracy(labels_oh, outputs))\n",
        "\n",
        "        history.append({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": np.mean(train_losses),\n",
        "            \"train_accuracy\": np.mean(train_accuracies),\n",
        "            \"val_loss\": np.mean(val_losses),\n",
        "            \"val_accuracy\": np.mean(val_accuracies)\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            \"Epoch:\", epoch + 1,\n",
        "            \"Train Acc:\", history[-1][\"train_accuracy\"],\n",
        "            \"Val Acc:\", history[-1][\"val_accuracy\"]\n",
        "        )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "wUTUavK-DrMS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.EXPERIMENTS"
      ],
      "metadata": {
        "id": "IhGy33BCDuap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiments = [\n",
        "    {\n",
        "        \"layers\": [784, 128, 10],\n",
        "        \"activations\": [\"relu\", \"softmax\"]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [784, 256, 128, 10],\n",
        "        \"activations\": [\"relu\", \"relu\", \"softmax\"]\n",
        "    },\n",
        "    {\n",
        "        \"layers\": [784, 128, 64, 10],\n",
        "        \"activations\": [\"tanh\", \"tanh\", \"softmax\"]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "cyyjoeA_Dwkh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for exp in experiments:\n",
        "    model = NeuralNetwork(\n",
        "        layer_sizes=exp[\"layers\"],\n",
        "        activations=exp[\"activations\"],\n",
        "        learning_rate=0.01\n",
        "    )\n",
        "\n",
        "    history = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        epochs=10\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF6KG4BnDz9h",
        "outputId": "753dba22-767a-4890-faab-f18b545a6504"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Train Acc: 0.6240171908315565 Val Acc: 0.7915007961783439\n",
            "Epoch: 2 Train Acc: 0.836004131130064 Val Acc: 0.8714171974522293\n",
            "Epoch: 3 Train Acc: 0.8773320895522388 Val Acc: 0.8921178343949044\n",
            "Epoch: 4 Train Acc: 0.8933069029850746 Val Acc: 0.9015724522292994\n",
            "Epoch: 5 Train Acc: 0.9005863539445629 Val Acc: 0.9074442675159236\n",
            "Epoch: 6 Train Acc: 0.9055670309168443 Val Acc: 0.9126194267515924\n",
            "Epoch: 7 Train Acc: 0.9105143923240938 Val Acc: 0.9168988853503185\n",
            "Epoch: 8 Train Acc: 0.9138792643923241 Val Acc: 0.9200835987261147\n",
            "Epoch: 9 Train Acc: 0.9177605277185501 Val Acc: 0.9231687898089171\n",
            "Epoch: 10 Train Acc: 0.920092617270789 Val Acc: 0.9244625796178344\n",
            "Epoch: 1 Train Acc: 0.11222348081023455 Val Acc: 0.11355493630573249\n",
            "Epoch: 2 Train Acc: 0.1382762526652452 Val Acc: 0.2695063694267516\n",
            "Epoch: 3 Train Acc: 0.39835421108742003 Val Acc: 0.6192277070063694\n",
            "Epoch: 4 Train Acc: 0.722581289978678 Val Acc: 0.7886146496815286\n",
            "Epoch: 5 Train Acc: 0.8109841417910447 Val Acc: 0.839171974522293\n",
            "Epoch: 6 Train Acc: 0.8548773987206824 Val Acc: 0.8725119426751592\n",
            "Epoch: 7 Train Acc: 0.876948960554371 Val Acc: 0.8829617834394905\n",
            "Epoch: 8 Train Acc: 0.8904084488272921 Val Acc: 0.8946058917197452\n",
            "Epoch: 9 Train Acc: 0.8989039179104478 Val Acc: 0.9004777070063694\n",
            "Epoch: 10 Train Acc: 0.9054337686567164 Val Acc: 0.9069466560509554\n",
            "Epoch: 1 Train Acc: 0.11147388059701492 Val Acc: 0.11355493630573249\n",
            "Epoch: 2 Train Acc: 0.14084155117270789 Val Acc: 0.33419585987261147\n",
            "Epoch: 3 Train Acc: 0.43996535181236673 Val Acc: 0.6114649681528662\n",
            "Epoch: 4 Train Acc: 0.6681769722814499 Val Acc: 0.7317874203821656\n",
            "Epoch: 5 Train Acc: 0.7768690031982942 Val Acc: 0.8179737261146497\n",
            "Epoch: 6 Train Acc: 0.8310567697228145 Val Acc: 0.8474323248407644\n",
            "Epoch: 7 Train Acc: 0.8557269456289979 Val Acc: 0.8659434713375797\n",
            "Epoch: 8 Train Acc: 0.8727511993603412 Val Acc: 0.8783837579617835\n",
            "Epoch: 9 Train Acc: 0.8850613006396588 Val Acc: 0.8898288216560509\n",
            "Epoch: 10 Train Acc: 0.894589552238806 Val Acc: 0.8992834394904459\n"
          ]
        }
      ]
    }
  ]
}